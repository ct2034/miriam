{
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Some setting up stuff. This cell is hidden from the Sphinx-rendered documentation.\n",
      "%matplotlib inline\n",
      "%config InlineBackend.figure_format = 'png'\n",
      "np.random.seed(42)"
     ],
     "language": "python",
     "metadata": {
      "nbsphinx": "hidden"
     },
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Maximum likelihood: Gaussian process hyperparameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**NOTE:** *Maximum likelihood estimation is very experimental in BayesPy. The whole messaging system is being rewritten in order to support much better MLE along with non-conjugate distributions and a bunch of other useful features. This example just shows how to do MLE currently if you need to but it doesn't work very well (the optimizer is bad).*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create some 3-dimensional inputs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import numpy as np\n",
      "N = 200\n",
      "X = np.random.randn(N, 1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pre-compute a squared distance matrix for the inputs:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from scipy.spatial import distance\n",
      "D = distance.squareform(distance.pdist(X, 'sqeuclidean'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a covariance function (exponentiated square or squared exponential):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def exponentiated_square(parameters):\n",
      "    lengthscale = np.exp(parameters[0])\n",
      "    magnitude = np.exp(parameters[1])\n",
      "    return magnitude**2 * np.exp(-D/lengthscale) + 1e-6 * np.identity(N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Implement the backward gradient pass for the exponentiated square:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def d_parameters(d_exponentiated_square, parameters):\n",
      "    \"\"\" Backward gradient of exponentiated square w.r.t. the parameters \"\"\"\n",
      "    lengthscale = np.exp(parameters[0])\n",
      "    magnitude = np.exp(parameters[1])\n",
      "    K = magnitude**2 * np.exp(-D/lengthscale)\n",
      "    return [\n",
      "        np.sum(d_exponentiated_square * K * D / lengthscale),\n",
      "        np.sum(d_exponentiated_square * K * 2)\n",
      "    ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "BayesPy uses precision matrix instead of covariance matrix for the Gaussian variables, thus we need to implement the matrix inversion and its gradient:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def inverse(K):\n",
      "    return np.linalg.inv(K)\n",
      "\n",
      "\n",
      "def d_covariance(d_inverse, K):\n",
      "    \"\"\" Backward gradient of inverse w.r.t. the covariance matrix \"\"\"\n",
      "    invK = np.linalg.inv(K)\n",
      "    return -np.dot(invK, np.dot(d_inverse, invK))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a maximum likelihood node for the covariance hyperparameters. Because the maximum likelihood estimation assumes unbounded variables, the node represents the parameters in log-scale:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import bayespy as bp\n",
      "parameters = bp.nodes.MaximumLikelihood(np.log([1, 1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create nodes that use our defined functions to compute the precision matrix from the parameters. `Function` takes the actual function as the first argument and then a 2-tuple for each input argument of the function. The first tuple elements are the input nodes and the second tuple elements are the corresponding gradient pass functions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "Covariance = bp.nodes.Function(\n",
      "    exponentiated_square,\n",
      "    (parameters, d_parameters)\n",
      ")\n",
      "Lambda = bp.nodes.Function(\n",
      "    inverse,\n",
      "    (Covariance, d_covariance)\n",
      ")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Create a noiseless latent Gaussian process node::"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "latent = bp.nodes.Gaussian(np.zeros(N), Lambda)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Observation noise precision:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tau = bp.nodes.Gamma(1e-3, 1e-3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Node for the observations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "Y = bp.nodes.GaussianARD(latent, tau)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Draw a sample from our model and use it as a data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "K = exponentiated_square(np.log([0.3, 5]))\n",
      "data = bp.nodes.Gaussian(np.zeros(N), np.linalg.inv(K + 0.1**2*np.identity(N))).random()\n",
      "Y.observe(data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Construct inference engine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "Q = bp.inference.VB(Y, latent, tau, parameters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Use gradient based optimization to learn the parameters. We collapse `latent` and `tau` so that the learning is more efficient because the coupling between `latent` and `parameters` is quite strong."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "Q.optimize(parameters, collapsed=[latent, tau], maxiter=100, verbose=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Show the learned parameters:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(\"Learned GP parameters:\", np.exp(parameters.get_moments()[0]))\n",
      "print(\"Learned noise std:\", tau.get_moments()[0] ** (-0.5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Simple plot of a posterior sample of the latent function values:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "plt.plot(X[:,0], latent.random(), 'k.');"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}
